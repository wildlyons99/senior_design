{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These imports should run successfully (with a warning you can ignore). If they don't, it means you need to install them with the command listed on the lab handout! Close out of Jupyter first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifier Perceptron Model\n",
    "\n",
    "In this notebook, we'll write the essential functions to build and train a Perceptron that can classify images in the MNIST dataset. If you've never used Jupyter, these boxes are individual code snippets that can be run individually. Variables are stored in their scope across the entire document. Boxes like these are markdown, for documentation and instructional purposes!\n",
    "\n",
    "### I. Perceptron Functions\n",
    "\n",
    "To start, we'll write the functions that define a Binary Classifying Perceptron. Looking at Figure 1 in the handout, we can see that the overall behavior is split into two operations: the dot product in the neuron, and the activation function. Start by implementing a function that recreates the neuron. This function should compute the dot product between our input and weight vectors, then add the bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(x, weights, bias):\n",
    "    \"\"\"\n",
    "    Computes the dot product between given weights and data vector (x), then adds bias.\n",
    "    \"\"\"\n",
    "    # your code here!\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the activation function that will map the dot product to either a '1' or a '0' (as seen in Figure 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(dot_product):\n",
    "    \"\"\"\n",
    "    In this simple model, the activation function is a unit step which classifies\n",
    "    negative inputs as 0 and positive inputs as 1.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return binary_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions together define the entirety of the Perceptron's behavior. Putting them together in a 'predict' function gives us the ability (with the correct weights) to classify vectors of pixels as either '1's or '0's!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, weights, bias):\n",
    "    \"\"\"\n",
    "    Predicts the class of a given data point (x) by running the input through \n",
    "    the neuron (dot product) and then applying the activation function.\n",
    "    \"\"\"\n",
    "    # your code here!\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Training Functions\n",
    "\n",
    "Next, we will implement some functions that will train the weights of our model to make correct predictions. The key function is 'foward_propagation', which acts on a single input vector and returns the loss. We're using squared error loss (for a single input, just the squared difference between the prediction and the label) and its derivative.\n",
    "\n",
    "Try writing the forward_propagation function here. You will need to run the model (weights and bias) on the input x, and compare it to the label y. You should output 1) the predicted value, 2) the squared error, and 3) the derivative of the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, y, weights, bias): \n",
    "    \"\"\"\n",
    "    x: training data as a vector (nparray), where each value corresponds\n",
    "        to a feature's value\n",
    "    y: label (0 or 1)\n",
    "    weights: weights of the perceptron\n",
    "    bias: bias\n",
    "    \"\"\"\n",
    "    y_pred =  # your code here! \n",
    "    loss   =  # your code here!\n",
    "    d_loss =  # your code here!\n",
    "    \n",
    "    return y_pred, loss, d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training, each of the weights is adjusted by the partial derivative of the loss with respect to that weight. In 'back_propagation', we must now take the partial derivative (with respect to each weight) of the dot product performed in the neuron. We are recreating the equation described in the last section:\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_i} = \\frac{\\partial C}{\\partial y} * \\frac{\\partial y}{\\partial z} * \\frac{\\partial z}{\\partial w_i} \n",
    "$$\n",
    "In forward_propagation, we have found the first term $\\frac{\\partial C}{\\partial y}$. It turns out we can ignore the second term $\\frac{\\partial y}{\\partial z}$ (the derivative of the prediction y with respect to the dot product z) because the step function is not differentiable. We need to find the last term $\\frac{\\partial z}{\\partial w_i}$, then multiply it by the result from forward_propagation to get the full partial derivative.\n",
    "\n",
    "Try writing back_propagation here. The function takes the input vector x and the derivative of the loss as found from forward_propagation. The derivative of the dot product with respect to each weight is deceptively simple. The function should return that derivative, multiplied by the d_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(x, d_loss):\n",
    "    \"\"\"\n",
    "    Performs the Backpropagation step on a given data point.\n",
    "    Receives as input the data point, the Perceptron's weights and the partial derivative of the loss\n",
    "    over the predicted y.\n",
    "    The received derivative is used to calculate the partial derivative of the loss over the weight of each feature.\n",
    "    A list with the partial derivatives of the loss over each weight is returned.\n",
    "    \"\"\"\n",
    "    partial_derivates = list()\n",
    "    for feature_value in x:\n",
    "        partial_derivates.append(# your code here!)\n",
    "        \n",
    "    return partial_derivates  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final training function brings it all together. In this function, 'x' is a 2D array of pixel vectors containing our entire dataset and 'y' is a vector containing the correct label for each image. To optimize the perceptron, we run through the dataset numerous times and adjust the weights by those partial derivatives until the error is sufficiently minimized. Using the functions you wrote, fill in the missing sections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_perceptron(x, y, learning_rate,  maxEpochs):\n",
    "    \"\"\"\n",
    "    Optimizes the Perceptron's weights by looping over the same steps for the specified number of epochs.\n",
    "    Steps:\n",
    "    1. Forward propagate data point\n",
    "    2. Backpropagate\n",
    "    3. Update weights\n",
    "    4. Check stop conditions while looping\n",
    "    \"\"\"\n",
    "    epoch = 0\n",
    "    error = 999\n",
    "    weights = np.random.rand(x.shape[1])\n",
    "    bias = 0 # we can just keep the bias at zero\n",
    "    \n",
    "    errors = list()\n",
    "    epochs = list()\n",
    "    \n",
    "    # Loop until stop conditions are met\n",
    "    while (epoch <= maxEpochs) and (error > 9e-4):\n",
    "        \n",
    "        loss_ = 0\n",
    "        # Loop over every data point\n",
    "        for i in range(x.shape[0]):\n",
    "            \n",
    "            # Forward Propagation on each data point\n",
    "            y_pred, loss, d_loss = # YOUR CODE HERE!\n",
    "\n",
    "            # Backpropagation\n",
    "            partial_derivates = # YOUR CODE HERE\n",
    "            \n",
    "            # Learn by updating the weights of the perceptron\n",
    "            weights = # YOUR CODE HERE! (not a function you wrote, extra challenging)\n",
    "\n",
    "        # Evaluate the results\n",
    "        for index, feature_value_test in enumerate(x):\n",
    "            y_pred, loss, d_loss = forward_propagation(feature_value_test, y[index], weights, bias)\n",
    "            loss_ += loss\n",
    "\n",
    "        errors.append(loss_/len(x))\n",
    "        epochs.append(epoch)\n",
    "        error = errors[-1]\n",
    "        epoch += 1\n",
    "\n",
    "        print('Epoch {}. loss: {}'.format(epoch, errors[-1]))\n",
    "\n",
    "    \n",
    "    return weights, bias, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're finally ready to get our data! We'll be using just the 0s and 1s from the MNIST dataset, a selection of handwritten digits used frequently as a benchmark for neural networks. This section is pre-written for you, but take a second to see how the data is formatted and split in training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_mnist.zip')\n",
    "\n",
    "# Take only data with labels 1\n",
    "data_ones = data[data['label'] == 1]\n",
    "\n",
    "# Take only data with labels 0\n",
    "data_zeros = data[data['label'] == 0]\n",
    "\n",
    "# If you want to change either digit, just change the label value to either a 0 or a 1 so the activation function is still valid\n",
    "# Ex. data_sevens = data[data['label'] == 7]\n",
    "#     data_sevens['label'] = 0\n",
    "\n",
    "# Concatenate instances with label 0 and 1\n",
    "data = pd.concat([data_ones, data_zeros])\n",
    "print(\"Data Matrix: {}\".format(data.shape))\n",
    "print(\"Data Labels: {}\".format(np.unique(data['label'].to_numpy())))\n",
    "\n",
    "# Split dataset with 75% training data and 25% test data\n",
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=1, shuffle=True)\n",
    "\n",
    "# Split datasets into features and labels\n",
    "x_train = train_data.drop('label', axis=1).to_numpy()\n",
    "x_test = test_data.drop('label', axis=1).to_numpy()\n",
    "y_train = train_data['label'].to_numpy()\n",
    "y_test = test_data['label'].to_numpy()\n",
    "\n",
    "# Let's see our data! The second dimension is the number of pixels, 28x28 = 784\n",
    "print(\"Training Set Matrix: {}\".format(x_train.shape))\n",
    "print(\"Test Set Matrix: {}\".format(x_test.shape))\n",
    "print(\"Training Label Matrix: {}\".format(y_train.shape))\n",
    "print(\"Test Set Matrix: {}\".format(y_test.shape))\n",
    "\n",
    "# Rescale data points to values between 0 and 1 (pixels are originally 0-255)\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# An example image from the training set:\n",
    "plt.imshow(x_train[1].reshape([28, 28]), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Training the Model\n",
    "\n",
    "We have our data, and we have our training functions. Let's run optimize_perceptron with these parameters and see how the model behaves. You should see the loss decreasing after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias, errors = # your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we turn these values into fixed-point bytes, we want them to be between 1 and -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(weights)):\n",
    "    if weights[i] > 0:\n",
    "        weights[i] = weights[i] / max(weights)\n",
    "    if weights[i] < 0:\n",
    "        weights[i] = -( weights[i] / min(weights) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Evaluating the Model\n",
    "\n",
    "We'll use this function to determine the accuracy of our model. It's prewritten, just loops through the entire test set and finds the ratio of correct predictions to total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(x_test, y_test, weights, bias):\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    for sample, label in zip(x_test, y_test):\n",
    "        prediction = predict(sample, weights, bias)\n",
    "\n",
    "        if prediction == label:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(x_test)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "acc = calculate_accuracy(x_test, y_test, weights, bias)\n",
    "print('Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Extracting Weights and Images\n",
    "\n",
    "Visualizing the weights of the Perceptron show how it has learned - you may see an abstract shape of a 0 or 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(weights.reshape([28,28]), cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final goal of this notebook is to port the weights into our VHDL module as ROM. To do this, we'll need to convert all the weights into signed 8-bit values. Then, we'll create a .vhd file with a case statement containing all these weights.\n",
    "\n",
    "First, we need a way to turn those decimal values into 8-bit signed types. Give it your best shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twos_comp(num, bits):\n",
    "    \"\"\"Converts an integer to its two's complement binary representation as a string.\n",
    "        num: The integer to convert.\n",
    "        bits: The number of bits to use for the representation.\n",
    "    \"\"\"\n",
    "    # your code here!\n",
    "    return binary\n",
    "\n",
    "# Here, we turn all the weights into 8-bit types using your function. Note that first we're scaling the -1:1 decimal values up to -127:127, for 8 bit representation.\n",
    "weight_values = []\n",
    "for w in weights:\n",
    "    weight_val = int(w * 127)\n",
    "    weight_values.append(twos_comp(weight_val, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate a .vhd file containing all these weights, as a ROM (Read-Only Memory). This section is pretty tedious, so most of it has been written for you. You'll still need to generate the correct addresses and values for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"weight_rom.vhd\", 'w')\n",
    "\n",
    "fileWidth = 28\n",
    "xCoordinate = -1\n",
    "yCoordinate = 0\n",
    "\n",
    "file.write('library IEEE;\\nuse IEEE.std_logic_1164.all;\\nuse IEEE.numeric_std.all;\\n')\n",
    "file.write('entity weight_rom is\\n')\n",
    "file.write('    port(\\n')\n",
    "file.write('        clk : in std_logic;\\n')\n",
    "file.write('        totaladr: in unsigned(9 downto 0);\\n')\n",
    "file.write('        weight : out signed(7 downto 0)\\n')\n",
    "file.write('        );\\n')\n",
    "file.write('end weight_rom;\\n')\n",
    "file.write('\\n')\n",
    "file.write('architecture synth of weight_rom is\\n')\n",
    "file.write('begin\\n')\n",
    "file.write('    process (clk) begin\\n')\n",
    "file.write('        if rising_edge(clk) then\\n')\n",
    "file.write('            case totaladr is\\n')\n",
    "\n",
    "# loop through all weight values and adjust x and y coordinates\n",
    "for i in range(len(weight_values)):\n",
    "    # your code here!\n",
    "\n",
    "    addr = # your code here!\n",
    "\n",
    "    value = # your code here!\n",
    "    file.write('                when \"' + addr + '\" => weight <= \"' + value + '\";\\n')\n",
    "    \n",
    "file.write('                when others => weight <= \"00000000\";\\n')\n",
    "\n",
    "file.write('        end case;\\n')\n",
    "file.write('    end if;\\n')\n",
    "file.write('    end process;\\n')\n",
    "file.write('end;\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the easiest way to load images onto the FPGA is through the same method. So each image will have its own ROM and .vhd file, generated with nearly the exact same script as above. This section is entirely prewritten, so you are welcome to look at a solution for the previous section and possibly replace it with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "image = x_train[i]\n",
    "\n",
    "plt.imshow(image.reshape([28, 28]), cmap='Greys_r')\n",
    "print(f'This image is a {y_train[i]}')\n",
    "\n",
    "filename = f'im{i}_{y_train[i]}.vhd'\n",
    "\n",
    "file = open(filename, 'w')\n",
    "\n",
    "fileWidth = 28\n",
    "xCoordinate = -1\n",
    "yCoordinate = 0\n",
    "\n",
    "file.write('library IEEE;\\nuse IEEE.std_logic_1164.all;\\nuse IEEE.numeric_std.all;\\n')\n",
    "file.write(f'entity im{i}_{y_train[i]} is\\n')\n",
    "file.write('    port(\\n')\n",
    "file.write('        clk : in std_logic;\\n')\n",
    "file.write('        totaladr: in unsigned(9 downto 0);\\n')\n",
    "file.write('        grayScale : out signed(7 downto 0)\\n')\n",
    "file.write('        );\\n')\n",
    "file.write(f'end im{i}_{y_train[i]};\\n')\n",
    "file.write('\\n')\n",
    "file.write(f'architecture synth of im{i}_{y_train[i]} is\\n')\n",
    "file.write('begin\\n')\n",
    "file.write('    process (clk) begin\\n')\n",
    "file.write('        if rising_edge(clk) then\\n')\n",
    "file.write('            case totaladr is\\n')\n",
    "\n",
    "print(len(image))\n",
    "\n",
    "pixel_values = []\n",
    "for p in range(len(image)):\n",
    "    xCoordinate += 1\n",
    "    if xCoordinate == fileWidth:\n",
    "        yCoordinate += 1\n",
    "        xCoordinate = 0\n",
    "\n",
    "    addr = str(f'{yCoordinate:05b}') + str(f'{xCoordinate:05b}')\n",
    "\n",
    "\n",
    "    pixel_val = int(image[p] * 127)\n",
    "    pixel_values.append(twos_comp(pixel_val, 8))\n",
    "\n",
    "    color = str(f'{pixel_values[p]}')\n",
    "    \n",
    "    file.write('                when \"' + addr + '\" => grayScale <= \"' + color + '\";\\n')\n",
    "file.write('                when others => grayScale <= \"00000000\";\\n')\n",
    "\n",
    "file.write('        end case;\\n')\n",
    "file.write('    end if;\\n')\n",
    "file.write('    end process;\\n')\n",
    "file.write('end;\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. FAKE SECTION: debugging\n",
    "\n",
    "It turns out that python is pretty bad at binary multiplication. Here's a section devoted to the real debugging process in which we use those binary values from the .vhd, convert them back to integers, and print the decimal and binary output from every operation in the perceptron. For bugfixing! Python can't multiply in binary on its own, so lots of conversion between binary and integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_predict(x, weights, bias):\n",
    "    \"\"\"\n",
    "    Predicts the class of a given data point (x) by running the input through \n",
    "    the neuron (dot product) and then applying the activation function.\n",
    "    \"\"\"\n",
    "    prediction = np.dot(weights, x) + bias\n",
    "    print(prediction)\n",
    "    prediction = activation_function(prediction)\n",
    "    \n",
    "    return (prediction)\n",
    "\n",
    "debug_predict(x_train[i], weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes you want to see more than just the final output. If you want to see the result of the dot product in binary and decimal for every single pixel, now you can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal(bin, dec):\n",
    "    \"\"\"Converts a two's complement binary str representation into a decimal w/ a specified point.\n",
    "        bin: The binary to convert.\n",
    "        dec: The bit location of the decimal point\n",
    "    \"\"\"\n",
    "    decimal_value = 0\n",
    "    for i, digit in enumerate(bin):\n",
    "        if digit == '1':\n",
    "            if(i == 0):\n",
    "                decimal_value -= 1\n",
    "            else:\n",
    "                decimal_value += 2 ** (dec - i - 1)\n",
    "    return decimal_value\n",
    "\n",
    "def sign_extend(bin, length):\n",
    "    \"\"\"Sign extends a binary number to a certain length.\n",
    "        bin: The binary to extend.\n",
    "        length: The length of new binary.\n",
    "    \"\"\"\n",
    "    extended_bin = ''\n",
    "    bin_len = len(bin)\n",
    "    for i in range(bin_len):\n",
    "        if i == 0:\n",
    "            for d in range(length - bin_len):\n",
    "                extended_bin += bin[i]\n",
    "        extended_bin += bin[i]\n",
    "    return extended_bin\n",
    "\n",
    "sum = 0\n",
    "sum_dec = 0\n",
    "for i in range(len(weight_values)):\n",
    "    pixel = int(sign_extend(pixel_values[i], 16), 2) # convert to int (sign extend for signed multiplication!)\n",
    "    w = int(sign_extend(weight_values[i], 16), 2)\n",
    "    prod = (pixel * w) # take product\n",
    "    prod_bin = f\"{prod:016b}\"\n",
    "    prod_bin = prod_bin[len(prod_bin) - 16:]\n",
    "    sum += int(sign_extend(prod_bin, 20), 2) # sign extend AGAIN for the larger sum register (20 bits)\n",
    "\n",
    "\n",
    "    sum_bin = f\"{sum:020b}\"\n",
    "    sum_bin = sum_bin[len(sum_bin) - 20:]\n",
    "\n",
    "    pixel_dec = decimal(pixel_values[i], 1)\n",
    "    w_dec = decimal(weight_values[i], 1)\n",
    "    prod_dec = pixel_dec * w_dec\n",
    "    sum_dec += prod_dec\n",
    "    \n",
    "    print(f'PIXEL: {pixel_values[i]}, {pixel_dec:0f}, WEIGHT: {weight_values[i]}, {w_dec:0f}, PRODUCT: {prod_bin}, {prod_dec:0f}, SUM: {sum_bin}, {sum_dec}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
